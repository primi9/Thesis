{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP5vc+6V3HLexm/Wz7to8yW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import zipfile, os, urllib.request, glob, math, shutil, sys\n","import torchvision\n","import torch.optim as optim\n","from torchvision import datasets, models, transforms\n","from sklearn.metrics import confusion_matrix, balanced_accuracy_score, precision_recall_fscore_support, roc_auc_score, accuracy_score\n","import sklearn.metrics\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import PIL\n","import PIL.Image\n","\n","sys.path.append('drive/MyDrive')\n","import modelArchs\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"sUEz2kFwoKwr","executionInfo":{"status":"ok","timestamp":1693919805403,"user_tz":-180,"elapsed":25159,"user":{"displayName":"Θανάσης Πριμικύρης","userId":"14742434534342737133"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e76b271-9026-489b-d156-31fa4df1999c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Κάνω load τα μοντέλα."],"metadata":{"id":"JcdUVtYtoM4A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5SwtAqDQjp1y"},"outputs":[],"source":["#load resnet34\n","metaModel_resnet34 = modelArchs.get_metaModel_Resnet(modelArchs.get_Resnet(8),11,8)\n","metaModel_resnet34.to(device)\n","metaModel_resnet34.load_state_dict(torch.load('drive/MyDrive/resnet34_metadata_derm.pt')['model'])\n","metaModel_resnet34.eval()\n","\n","#load mobilenet\n","metaModel_mobilenet_derm = modelArchs.get_metaModel_MobileNets(modelArchs.get_MobileNet(8),11,8)\n","metaModel_mobilenet_derm.to(device)\n","metaModel_mobilenet_derm.load_state_dict(torch.load('drive/MyDrive/mobilenet_metadata_derm.pt')['model'])\n","metaModel_mobilenet_derm.eval()\n","\n","#load efficient net\n","metaModel_effnet_derm = modelArchs.get_metaModel_EfficientNet(modelArchs.get_EfficientNet(8),11,8)\n","metaModel_effnet_derm.to(device)\n","metaModel_effnet_derm.load_state_dict(torch.load('drive/MyDrive/effnet_metadata_derm.pt')['model'])\n","metaModel_effnet_derm.eval()\n","\n","#load googlenet\n","metaModel_googlenet_derm = modelArchs.get_metaModel_GoogleNet(modelArchs.get_GoogleNet(8),11,8)\n","metaModel_googlenet_derm.to(device)\n","metaModel_googlenet_derm.load_state_dict(torch.load('drive/MyDrive/googlenet_metadata_derm.pt')['model'])\n","metaModel_googlenet_derm.eval()\n","\n","#load mobilenet + sa\n","metaModel_mobilenet_sa_derm = modelArchs.get_metaModel_MobileNets(modelArchs.get_MobileNetSa(8),11,8)\n","metaModel_mobilenet_sa_derm.to(device)\n","metaModel_mobilenet_sa_derm.load_state_dict(torch.load('drive/MyDrive/mobilenet_sa_metadata_derm.pt')['model'])\n","metaModel_mobilenet_sa_derm.eval()"]},{"cell_type":"markdown","source":["\n","\n","Οι εικόνες είναι αποθηκευμένες στο drive σε zip.\n","\n","Υπάρχουν 16 zip files, ένα για κάθε κλάση από το train και test dataset.\n","\n","Κάνω extract τις εικόνες.\n"],"metadata":{"id":"G1ihGdLHoR7b"}},{"cell_type":"code","source":["os.makedirs('test_imgs')\n","\n","prefix = 'drive/MyDrive/'\n","\n","for i in ['test_imgs']:\n","  for j in ['MEL','NV','BCC','AK','BKL','DF','VASC','SCC']:\n","\n","    name = i + '_' + j\n","    src_zip = prefix + name + '.zip'\n","    dest_zip = i + '/' + j\n","\n","    with zipfile.ZipFile(src_zip, 'r') as zip_ref:\n","      zip_ref.extractall(dest_zip)\n","\n","    print(\"Finished with: \", name)"],"metadata":{"id":"GFN999tsoQNd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Χρήση των transformation που χρησιμοποιήθηκαν στο testing των μοντέλων."],"metadata":{"id":"dLz4O0AmoUNI"}},{"cell_type":"code","source":["transforms_val = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","val_dataset = pd.read_csv('drive/MyDrive/test_dataset_meta_derm.csv',converters={'sex': pd.eval, 'anatom_site_general': pd.eval, 'age_approx': pd.eval})\n","print(len(val_dataset))"],"metadata":{"id":"adJGnWn9oUjN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load τα μεταδεδομένα."],"metadata":{"id":"gqNXlYq7pHJ7"}},{"cell_type":"code","source":["features_list = ['age_approx', 'anatom_site_general', 'sex']\n","\n","class ImageFolderWithFileNames(datasets.ImageFolder):\n","\n","    def __init__(self, root_dir, train_transformations, current_dataset, features_list):\n","\n","      super().__init__(root_dir,train_transformations)\n","\n","      self.current_dataset = current_dataset\n","      self.features_list = features_list\n","\n","    def __getitem__(self, index):\n","\n","      path, target = self.imgs[index]\n","\n","      filename = path[path.rfind('/') + 1:]\n","      metadata_features = self.get_metadata_features(filename)\n","\n","      img = self.loader(path)\n","      if self.transform is not None:\n","        img = self.transform(img)\n","      if self.target_transform is not None:\n","        target = self.target_transform(target)\n","\n","      return metadata_features,img,target\n","\n","    def get_metadata_features(self,filename):\n","\n","      features = []\n","\n","      index_row = self.current_dataset.index[self.current_dataset['image'] == filename[:-4]].tolist()[0]#get index of the row that corresponds to the image\n","\n","      feature_cols = self.current_dataset.iloc[index_row][self.features_list].to_numpy().flatten()\n","\n","      for i in feature_cols:\n","        for j in list(i):\n","          features.append(j)\n","\n","      return torch.tensor(features)\n","\n","batch_len = 64\n","val_dataset_imgs = ImageFolderWithFileNames('test_imgs', transforms_val, val_dataset, features_list)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset_imgs, batch_size = batch_len, shuffle=False)"],"metadata":{"id":"Bo07e5bIpGZZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Υπολογισμός των metrics, χρησιμοποιώντας average ensembling με τα μοντέλα που έκανα train."],"metadata":{"id":"nulf1d5Rogif"}},{"cell_type":"code","source":["def get_accuracy(logit, target, batch_size):\n","\n","  correct_classification = 0\n","\n","  for i in range(batch_size):\n","    if logit[i] == target[i]:\n","      correct_classification += 1\n","\n","  return (correct_classification / batch_size) * 100\n","\n","def get_predictions(models_out):\n","\n","  num_models = len(models_out)\n","\n","  #calculate softmax\n","  model_probs = []\n","  for i in range(num_models):\n","    temp_prob = torch.nn.functional.softmax(models_out[i],dim = 1)\n","    model_probs.append(temp_prob)\n","\n","  stacked_outs = torch.stack(model_probs, 0)\n","  mean_outs = torch.mean(stacked_outs, 0)\n","\n","  probs = torch.nn.functional.softmax(mean_outs, dim = 1).tolist()\n","  predictions = torch.max(mean_outs,1)[1].tolist()\n","\n","  return probs, predictions\n","\n","def test_ensemble(try_models, test_dataloader):\n","\n","  num_models = len(try_models)\n","\n","  test_acc = 0.0\n","  pred_list = []\n","  label_list = []\n","  output_probs = []\n","  metrics = []\n","\n","  for i, (metadata_features, inputs, labels) in enumerate(test_dataloader,1):\n","\n","    label_list.extend(labels.detach().tolist())\n","\n","    metadata_features = metadata_features.to(device)\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","\n","    models_out = []\n","    for j in range(num_models):\n","      temp_out = try_models[j](metadata_features,inputs).detach().cpu()\n","      models_out.append(temp_out)\n","\n","    probs, preds = get_predictions(models_out)\n","\n","    output_probs.extend(probs)\n","    pred_list.extend(preds)\n","\n","  conf_matrix = confusion_matrix(label_list , pred_list)\n","\n","  metrics.append(accuracy_score(label_list, pred_list))\n","  metrics.extend(precision_recall_fscore_support(label_list,pred_list, average = 'macro'))\n","  metrics.append(roc_auc_score(label_list,output_probs, average = 'macro', multi_class = 'ovr'))\n","  metrics.extend(precision_recall_fscore_support(label_list,pred_list, average = 'weighted'))\n","  metrics.append(roc_auc_score(label_list,output_probs, average = 'weighted', multi_class = 'ovr'))\n","\n","  return metrics, conf_matrix"],"metadata":{"id":"gTp7NBWxohJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#gia test dataset\n","models_try = [metaModel_resnet34, metaModel_mobilenet_derm, metaModel_googlenet_derm, metaModel_effnet_derm, metaModel_mobilenet_sa_derm]\n","metrics, conf_matrix = test_ensemble(models_try,val_dataloader)\n","print(metrics)\n","print(conf_matrix)"],"metadata":{"id":"_hzgFg7donk1"},"execution_count":null,"outputs":[]}]}