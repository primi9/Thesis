{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOjPKzzSlH3bg8gHWdF1qN9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9866480666204f619555d3862cac5b48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e1920586d364eb8bc17337f5cfb3fd9","IPY_MODEL_063c1d4077284d81af4d47c1ba541e90","IPY_MODEL_ff7ae4ca624c43e78175c448f5960848"],"layout":"IPY_MODEL_e9f28fa438e94ba3b850d5c226a20e28"}},"4e1920586d364eb8bc17337f5cfb3fd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a220ba4209e742aa96a3a0426b50232c","placeholder":"​","style":"IPY_MODEL_6e6e1a92c04542eb99f06cb288afa2fc","value":"100%"}},"063c1d4077284d81af4d47c1ba541e90":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_732a5600a73d4676913779688ec1a405","max":10306551,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2a5858f0e0b4940b1b690b4793f3d38","value":10306551}},"ff7ae4ca624c43e78175c448f5960848":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22d01aeb9b22488cbded95158235950a","placeholder":"​","style":"IPY_MODEL_83b3228962e940f9a7202844aaf086c6","value":" 9.83M/9.83M [00:00&lt;00:00, 47.7MB/s]"}},"e9f28fa438e94ba3b850d5c226a20e28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a220ba4209e742aa96a3a0426b50232c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e6e1a92c04542eb99f06cb288afa2fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"732a5600a73d4676913779688ec1a405":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2a5858f0e0b4940b1b690b4793f3d38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22d01aeb9b22488cbded95158235950a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83b3228962e940f9a7202844aaf086c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip3 install torch==1.13.0 torchvision torchaudio torchtext"],"metadata":{"id":"dc4LGTq8RUMr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694337189904,"user_tz":-180,"elapsed":129904,"user":{"displayName":"Θανάσης Πριμικύρης","userId":"14742434534342737133"}},"outputId":"459e0993-0c4a-4461-8c5e-780e0601ac51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==1.13.0\n","  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (4.5.0)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.41.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision\n","  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl (24.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchaudio\n","  Downloading torchaudio-2.0.2-cp310-cp310-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchaudio-2.0.1-cp310-cp310-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n","INFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchtext\n","  Downloading torchtext-0.15.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchtext-0.14.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchtext-0.14.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchvision, torchtext, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.1+cu118\n","    Uninstalling torch-2.0.1+cu118:\n","      Successfully uninstalled torch-2.0.1+cu118\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.15.2+cu118\n","    Uninstalling torchvision-0.15.2+cu118:\n","      Successfully uninstalled torchvision-0.15.2+cu118\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.15.2\n","    Uninstalling torchtext-0.15.2:\n","      Successfully uninstalled torchtext-0.15.2\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.0.2+cu118\n","    Uninstalling torchaudio-2.0.2+cu118:\n","      Successfully uninstalled torchaudio-2.0.2+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0 torchaudio-0.13.0 torchtext-0.14.0 torchvision-0.14.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mmGbfcnZMtkU","executionInfo":{"status":"ok","timestamp":1694337215544,"user_tz":-180,"elapsed":25659,"user":{"displayName":"Θανάσης Πριμικύρης","userId":"14742434534342737133"}},"outputId":"4b00e4cf-c9ff-4b39-9ba4-2a4cca4cd312"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import zipfile, os, urllib.request, glob, math, shutil, sys, random\n","import torchvision\n","import torch.optim as optim\n","from torchvision import datasets, models, transforms\n","from sklearn.metrics import confusion_matrix, balanced_accuracy_score, precision_recall_fscore_support, roc_auc_score, accuracy_score\n","import sklearn.metrics\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import PIL\n","import PIL.Image\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","source":["Extract IND and OOD images."],"metadata":{"id":"ir837CbOXwSb"}},{"cell_type":"code","source":["os.makedirs('train')\n","os.makedirs('test')\n","\n","prefix = 'drive/MyDrive/ood_classification/'\n","\n","for i in ['train', 'test']:\n","  for j in ['ind', 'ood']:\n","\n","    name = i + '_' + j\n","    src_zip = prefix + name + '.zip'\n","    dest_zip = i + '/' + j\n","\n","    with zipfile.ZipFile(src_zip, 'r') as zip_ref:\n","      zip_ref.extractall(dest_zip)\n","\n","    print(\"Finished with: \", name)\n","\n","!ls train/ind | wc -l\n","!ls test/ind | wc -l\n","!ls train/ood | wc -l\n","!ls test/ood | wc -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cq7XB14ZSgii","executionInfo":{"status":"ok","timestamp":1694337353223,"user_tz":-180,"elapsed":137720,"user":{"displayName":"Θανάσης Πριμικύρης","userId":"14742434534342737133"}},"outputId":"2708fa9a-8220-4ae2-fe46-5f93d5bc9ed2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished with:  train_ind\n","Finished with:  train_ood\n","Finished with:  test_ind\n","Finished with:  test_ood\n","1839\n","459\n","1839\n","459\n"]}]},{"cell_type":"markdown","source":["Κάνω load το μοντέλο που έγινε train χωρίς τα μετα-δεδομένα. Στη συνέχεια ορίζω το τελικό νευρωνικό δίκτυο(metaModel) που θα γίνει train στις εικόνες + μετα-δεδομένα."],"metadata":{"id":"SEowf4_mYEGv"}},{"cell_type":"code","source":["\"\"\"\n","from torchvision.models import mobilenet_v3_large\n","\n","model = mobilenet_v3_large(weights = 'DEFAULT')\n","model.classifier[3] = nn.Linear(1280, out_features = 256, bias = True)\n","#prostheto ena akoma linear layer gia to teleytaio classifier\n","model.classifier.add_module(\"4\", nn.Linear(256, 2, bias = True))\n","print(\"Test output vector: \", model(torch.rand(2,3,224,224)))\n","model.to(device)\n","\n","\"\"\"\n","from torchvision.models import mobilenet_v3_small\n","model = mobilenet_v3_small(weights = 'DEFAULT')\n","model.classifier[3] = nn.Linear(1024, out_features = 2, bias = True)\n","print(\"Test output vector: \", model(torch.rand(2,3,224,224)))\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9866480666204f619555d3862cac5b48","4e1920586d364eb8bc17337f5cfb3fd9","063c1d4077284d81af4d47c1ba541e90","ff7ae4ca624c43e78175c448f5960848","e9f28fa438e94ba3b850d5c226a20e28","a220ba4209e742aa96a3a0426b50232c","6e6e1a92c04542eb99f06cb288afa2fc","732a5600a73d4676913779688ec1a405","e2a5858f0e0b4940b1b690b4793f3d38","22d01aeb9b22488cbded95158235950a","83b3228962e940f9a7202844aaf086c6"]},"id":"LlRKidGCYAf6","executionInfo":{"status":"ok","timestamp":1694337366786,"user_tz":-180,"elapsed":6993,"user":{"displayName":"Θανάσης Πριμικύρης","userId":"14742434534342737133"}},"outputId":"de830af2-9fd9-47cc-e43b-677196aabc1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/9.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9866480666204f619555d3862cac5b48"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Test output vector:  tensor([[-0.0609, -0.0299],\n","        [-0.1314,  0.3091]], grad_fn=<AddmmBackward0>)\n"]},{"output_type":"execute_result","data":{"text/plain":["MobileNetV3(\n","  (features): Sequential(\n","    (0): Conv2dNormActivation(\n","      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","      (2): Hardswish()\n","    )\n","    (1): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n","          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (2): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n","          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (3): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n","          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (4): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (5): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (6): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (7): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n","          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (8): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (9): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n","          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (10): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (11): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (12): Conv2dNormActivation(\n","      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","      (2): Hardswish()\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=1)\n","  (classifier): Sequential(\n","    (0): Linear(in_features=576, out_features=1024, bias=True)\n","    (1): Hardswish()\n","    (2): Dropout(p=0.2, inplace=True)\n","    (3): Linear(in_features=1024, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Ορίζω διάφορα transformations για να κάνουμε augment το training set, ώστε να πετύχουμε καλύτερο generalization του μοντέλου. Εφαρμόζω πιο ισχυρά transformations από αυτά που εφάρμοσα στα training των ISIC,PAD."],"metadata":{"id":"hD9Qb6eLZ73z"}},{"cell_type":"code","source":["transforms_train = transforms.Compose([\n","    transforms.RandomResizedCrop(size = 224, scale = (0.4,1.0)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomVerticalFlip(),\n","    transforms.ColorJitter(brightness = [0.5, 1.5], contrast = [0.5, 1.5], saturation = [0.5, 1.5], hue = [-0.2,0.2]),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","transforms_test = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"1XqyPFMGZu32"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ορισμός υπερ-παραμέτρων."],"metadata":{"id":"ijOhxrM1aXuX"}},{"cell_type":"code","source":["epochs = 15\n","batch_len = 64\n","\n","criterion = nn.CrossEntropyLoss()\n","learning_rate = 0.001\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.2)"],"metadata":{"id":"7RBzmC_UaXcE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Εδώ ορίζω τα dataset και dataloaders, χρησιμοποιώντας το weightRandomSampler για να δημιουργήσει balanced batches:"],"metadata":{"id":"l1DaHD_QbR-h"}},{"cell_type":"code","source":["train_dataset = datasets.ImageFolder('train', transforms_train)\n","test_dataset = datasets.ImageFolder('test', transforms_test)\n","\n","len_train_dataset = len(train_dataset)\n","len_test_dataset = len(test_dataset)\n","\n","print(\"Length of train_dataset: \", len_train_dataset)\n","print(\"Length of test dataset: \", len_test_dataset)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_len, shuffle = True)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_len, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upKgUb2TbPZj","executionInfo":{"status":"ok","timestamp":1694337376987,"user_tz":-180,"elapsed":20,"user":{"displayName":"Θανάσης Πριμικύρης","userId":"14742434534342737133"}},"outputId":"4f6499ab-9f7a-4802-9929-b708ae0fdcd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of train_dataset:  3678\n","Length of test dataset:  918\n"]}]},{"cell_type":"markdown","source":["Ορίζω συναρτήσεις για υπολογισμό του accuracy και του confusion matrix πάνω στο test set. Στη συνέχεια θα δημιουργήσω και άλλες συναρτήσεις για υπολογισμό F1-score, sensitivity, specificity, precision."],"metadata":{"id":"lScpGLTObVIz"}},{"cell_type":"code","source":["def test_model(model,test_dataloader):\n","\n","  test_acc = 0.0\n","  pred_list = []\n","  label_list = []\n","  output_probs = []\n","  metrics = []\n","\n","  for i, (inputs, labels) in enumerate(test_dataloader,1):\n","\n","    label_list.extend(labels.data.tolist())\n","\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    outputs = model(inputs)\n","    probs = torch.softmax(outputs.data.cpu(), dim = 1)[:,1].tolist()\n","    output_probs.extend(probs)\n","\n","    pred =  torch.max(outputs, 1)[1].data.cpu().tolist()\n","    pred_list.extend(pred)\n","\n","  conf_matrix = confusion_matrix(label_list , pred_list)\n","\n","  metrics.append(accuracy_score(label_list, pred_list))\n","  metrics.extend(precision_recall_fscore_support(label_list,pred_list, average = 'macro'))\n","  metrics.append(roc_auc_score(label_list,output_probs, average = 'macro', multi_class = 'ovr'))\n","\n","  return metrics, conf_matrix"],"metadata":{"id":"xtfets_7bVsP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Το βασικό train loop."],"metadata":{"id":"jS4JQtKbbZIP"}},{"cell_type":"code","source":["for epoch in range(1, epochs + 1):\n","\n","  model.train()\n","  train_running_loss = 0\n","\n","  #Training phase\n","  print(\"Starting epoch: \" , epoch)\n","\n","  for i, (inputs, labels) in enumerate(train_dataloader,1):\n","      inputs = inputs.to(device)\n","      labels = labels.to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      train_running_loss += loss.detach().item()\n","\n","  print('Epoch: %d | Loss: %.4f ' %(epoch, train_running_loss / i))\n","  scheduler.step()\n","\n","  model.eval()\n","  try:\n","    metrics,conf_matrix = test_model(model,test_dataloader)\n","    print('Test metrics on epoch %d:' %(epoch))\n","    print(metrics)\n","    print(\"Confusion matrix: \")\n","    print(conf_matrix)\n","  except Exception as e:\n","    print(\"Error in calculating metrics\")\n","    print(e)\n","\n","  try:\n","    checkpoint = {'model': model.state_dict(), 'epoch' : epoch}\n","    checkpoint_fn = 'drive/MyDrive/ood_classification/checkpoint' + str(epoch) + '.pt'\n","    torch.save(checkpoint, checkpoint_fn)\n","  except Exception as e:\n","    print(\"Error in writing general checkpoint\")\n","    print(e)"],"metadata":{"id":"H1CaBFCebZZ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Δημιουργία για app."],"metadata":{"id":"1DkXUxtsYYeq"}},{"cell_type":"code","source":["print(torch.__version__)\n","print(torchvision.__version__)\n","\n","ood_classifier_app = mobilenet_v3_small()\n","ood_classifier_app.classifier[3] = nn.Linear(1024, out_features = 2, bias = True)\n","\n","ood_classifier_app.load_state_dict(torch.load('drive/MyDrive/ood_classification/checkpoint9.pt', map_location=torch.device(\"cpu\"))['model'])\n","ood_classifier_app.eval()\n","\n","traced_script_module = torch.jit.trace(ood_classifier_app, torch.rand(1,3,224,224))\n","traced_script_module._save_for_lite_interpreter(\"drive/MyDrive/ood_classification/ood_classifier.ptl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YqclcVqxYMY2","executionInfo":{"status":"ok","timestamp":1694338980067,"user_tz":-180,"elapsed":1524,"user":{"displayName":"Θανάσης Πριμικύρης","userId":"14742434534342737133"}},"outputId":"d8e05775-9df5-4b91-c876-533923252ef9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.13.0+cu117\n","0.14.0+cu117\n"]}]},{"cell_type":"markdown","source":["Τεστ τον DataLoader."],"metadata":{"id":"jgKx6XkXctLV"}},{"cell_type":"code","source":["from collections import Counter\n","\n","for inputs, labels in train_dataloader:\n","  inputs = inputs.to(device)\n","  print(\"Input size:\", inputs.shape)\n","  out = model(inputs)\n","  print(\"Output size: \", out.shape)\n","\n","  labels = labels.tolist()\n","  print(\"Test if batch is balanced: \")\n","  print(Counter(labels).keys())\n","  print(Counter(labels).values())\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNTBNgx6cVHU","executionInfo":{"status":"ok","timestamp":1694254595617,"user_tz":-180,"elapsed":6833,"user":{"displayName":"Θανάσης Πριμικύρης","userId":"14742434534342737133"}},"outputId":"0a7945f6-d618-4ffc-dd04-2a6e09606130"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input size: torch.Size([64, 3, 224, 224])\n","Output size:  torch.Size([64, 2])\n","Test if batch is balanced: \n","dict_keys([0, 1])\n","dict_values([34, 30])\n"]}]}]}